# Fine-Tuning BERT-based Transformers for Code Classification

# üîç CodeBERT vs BERT: Code Understanding Comparison

This project compares the performance of **CodeBERT** and **BERT** on a code understanding task. Both models are evaluated using key metrics including Accuracy, Precision, Recall, F1 Score, and Evaluation Loss.

## üìä Evaluation Results

| **Metric**     | **Accuracy** | **Precision** | **Recall** | **F1 Score** | **Eval Loss** |
|----------------|--------------|---------------|------------|--------------|----------------|
| **CodeBERT**   | 69.1%        | 65.6%         | 67.6%      | 65.1%        | 0.739          |
| **BERT**       | 66.0%        | 61.0%         | 62.1%      | 61.2%        | 0.873          |

## üß† Model Overview

- **BERT** is a general-purpose language model trained on natural language.
- **CodeBERT** builds on BERT but is trained with both programming languages (e.g., Python, Java) and natural language to better handle code-related tasks.


